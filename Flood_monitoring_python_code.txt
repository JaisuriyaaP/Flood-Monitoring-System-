#KNN
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
warnings.filterwarnings('ignore')
data = pd.read_csv("C:/Users/Jana.P.M/Desktop/IOT LAB/jcomp.csv")
data.head(10)
data.apply(lambda x:sum(x.isnull()), axis=0)
data['FLOODS'].replace(['YES','NO'],[1,0],inplace=True)
data.head(10)
x=data.iloc[:,1:14]
x.head(10)
y = data.iloc[:, -1]
y.head(10)
from sklearn import preprocessing
minmax = preprocessing.MinMaxScaler(feature_range=(0,1))
minmax.fit(x).transform(x)
from sklearn import model_selection,neighbors
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)
print(y_train.tail(10))
print(x_train.head(10))
clf = neighbors.KNeighborsClassifier()
knn_clf = clf.fit(x_train,y_train)
y_predict = knn_clf.predict(x_test)
print('predicted chances of flood')
print(y_predict)
print(y_test.values)
from sklearn.model_selection import cross_val_score
knn_accuracy =cross_val_score(knn_clf,x_test,y_test,cv=5,scoring='accuracy',n_jobs=1)
print(knn_accuracy)
print(knn_accuracy.mean())
# Logistic Regression
x_train_std = minmax.fit_transform(x_train)
x_test_std = minmax.transform(x_test)
print(x_train_std[:10])
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr_clf = lr.fit(x_train_std,y_train)
y_predict = lr_clf.predict(x_test_std)
print('Predicted values')
print(y_predict)
print('Actual values')
print(y_test.values)
lr_accuracy =cross_val_score(lr_clf,x_test_std,y_test,cv=5,scoring='accuracy',n_jobs=1)
print(lr_accuracy.mean())
print(lr_accuracy)
# Decision tree classification
from sklearn.tree import DecisionTreeClassifier
dtc_clf = DecisionTreeClassifier()
dtc_clf.fit(x_train,y_train)
dtc_clf_acc =
cross_val_score(dtc_clf,x_train_std,y_train,cv=3,scoring="accuracy",n_jobs=-1)
dtc_clf_acc
y_pred = dtc_clf.predict(x_test)
print('Predicted values')
print(y_pred)
print("actual values:")
print(y_test.values)
dtc_accuracy=cross_val_score(dtc_clf,x_test_std,y_test,cv=5,scoring='accuracy',n_jobs=-1)
print(dtc_accuracy.mean())
print(dtc_accuracy)
# Random forest classification
from sklearn.ensemble import RandomForestClassifier
rmf = RandomForestClassifier(max_depth=3,random_state=0)
rmf_clf = rmf.fit(x_train,y_train)
rmf_clf
y_pred = rmf_clf.predict(x_test)
print('Predicted values')
print(y_pred)
print("actual values:")
print(y_test.values)
rmf_clf_acc =
cross_val_score(rmf_clf,x_train_std,y_train,cv=5,scoring="accuracy",n_jobs=-1)
print(rmf_clf_acc.mean())
print(rmf_clf_acc)
models = []
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
models.append(('KNN', KNeighborsClassifier()))
models.append(('LR', LogisticRegression()))
models.append(('DT', DecisionTreeClassifier()))
models.append(('RF', RandomForestClassifier()))
names = []
scores = []
for name, model in models:
model.fit(x_train, y_train)
y_pred = model.predict(x_test)
scores.append(accuracy_score(y_test, y_pred))
names.append(name)
tr_split = pd.DataFrame({'Name': names, 'Accuracy Score': scores})
print(tr_split)
import seaborn as sns
axis = sns.barplot(x = 'Name', y = 'Accuracy Score', data =tr_split )
axis.set(xlabel='Classifier', ylabel='Accuracy')
for p in axis.patches:
height = p.get_height()
axis.text(p.get_x() + p.get_width()/2, height + 0.005, '{:1.4f}'.format(height), ha="center")
plt.show()